---
permalink: /nlp-seminar
---

<div class="container">
    <div class="row">
        <div class="col-lg-12 text-center">
    <h2>Natural Language Processing Seminar</h2>
    <h3>Winter Semester 2023/2024</h3>

    <b>Wednesdays, 10am -- Start Date: October 18, 2023</b><br/><br/>

    <i>Schedule under construction</i><br/><br/>

    <b>October 18, 2023</b><br/>
    <i>No meeting - ILKA talk by Flavia Sciolette at 5.30pm</i>
    <br/><br/>

    <b>October 25, 2023</b><br/>
    <i>No meeting - session has been preponed to Sept 27 (Lilian Wanzare)</i>
    <br/><br/>
    
    <b>Nov 1, 2023</b><br/>
    <i>Holiday</i>

    <br/><br/>

    <b>Nov 8, 2023</b><br/>Reading Session: Friedrich<br/>
    <i>This week, we will discuss two outstanding papers from ACL 2023.</i>
    <ul>
    <li><a href="https://aclanthology.org/2023.acl-long.213/">Entity Tracking in Language Models</a> (Kim & Schuster, ACL 2023)</li>
    <li><a hreF="https://aclanthology.org/2023.acl-long.697/">What’s the Meaning of Superhuman Performance in Today’s NLU?</a> (Tedeschi et al., ACL 2023)</li>
    </ul>

    <br/><br/>

    <b>Nov 15, 2023</b><br/>Reading Session: Zarcone<br/>

    <br/><br/>

    <b>Nov 22, 2023</b><br/>
    Guest talk by Sophie Henning (Bosch/LMU): Uncertainty and Confidence Modeling for Deep Learning in NLP
    <br/><br/>

    <b>Nov 29, 2023</b><br/>Reading Session: Chiarcos / Garcia<br/>

    <br/><br/>

    <b>Dec 6, 2023</b><br/>

    <br/><br/>

    <b>Dec 13, 2023</b><br/>

    <br/><br/>

    <b>Dec 20, 2023</b><br/>

    <br/><br/>

    <b>Jan 10, 2023</b><br/>

    <br/>Master thesis presentation: Steffen Kleinle<br/>

     <b>Jan 17, 2023</b><br/>

    <br/>PhD Update Reports: Zahra + Valentin<br/>

     <b>Jan 24, 2023</b><br/>

    <br/>PhD Update Reports: Timo + Wei<br/>

     <b>Jan 31, 2023</b><br/>

    <br/>External Guest Speaker: Diego Frassinelli (LMU München)
    
    <br/><br/>

    <b>Feb 7, 2023</b>Discussion + Breakfast<br/>

    <br/><br/>

    <b>Contact: Annemarie Friedrich (firstname dot lastname at uni minus a dot de)</b>

        </div>
    </div>
</div>
